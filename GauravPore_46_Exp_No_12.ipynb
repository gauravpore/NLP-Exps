{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "SanketChavan_05_Exp_No_12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGn0jKvLN9t"
      },
      "source": [
        "AIM:  Write a program to demonstrate use of various text similarity prediction algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uc8z4p0LN9v"
      },
      "source": [
        "Theory:\n",
        "Find and rank relevant content in Python using NLP, TF-IDF and GloVe.\n",
        "\n",
        "Term Frequency - inverse document frequency (TF-idf)\n",
        "\n",
        "Term Frequency-inverse document frequency (TF-idf): this looks at words that appear in both pieces of text, and scores them based on how often they appear. It is a useful tool if you expect the same words to appear in both pieces of text, but some words are more important that others.\n",
        "\n",
        "\n",
        "Semantic similarity, using GloVe word embeddings\n",
        "\n",
        "\n",
        "Semantic similarity: this scores words based on how similar they are, even if they are not exact matches. It borrows techniques from Natural Language Processing (NLP), such as word embeddings. This is useful if the word overlap between texts is limited, such as if you need ‘fruit and vegetables’ to relate to ‘tomatoes’\n",
        "\n",
        "Given a search query (text string) and a document corpus, these methods calculate a similarity metric for each document vs the query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ixIzpdLn_F"
      },
      "source": [
        "Execution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9AvBYn_LN9w"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "search_terms = 'fruit and vegetables'\n",
        "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
        "\n",
        "doc_vectors = TfidfVectorizer().fit_transform([search_terms] + documents)\n",
        "\n",
        "cosine_similarities = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()\n",
        "document_scores = [item.item() for item in cosine_similarities[1:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4aMEEChLN9x",
        "outputId": "db54452d-9735-4eec-81f9-9dfc6f7f7e9d"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords list\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Interface lemma tokenizer from nltk with sklearn\n",
        "class LemmaTokenizer:\n",
        "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
        "\n",
        "# Lemmatize the stop words\n",
        "tokenizer=LemmaTokenizer()\n",
        "token_stop = tokenizer(' '.join(stop_words))\n",
        "\n",
        "search_terms = 'red tomato'\n",
        "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
        "\n",
        "# Create TF-idf model\n",
        "vectorizer = TfidfVectorizer(stop_words=token_stop, \n",
        "                              tokenizer=tokenizer)\n",
        "doc_vectors = vectorizer.fit_transform([search_terms] + documents)\n",
        "\n",
        "# Calculate similarity\n",
        "cosine_similarities = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()\n",
        "document_scores = [item.item() for item in cosine_similarities[1:]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZebF2VfRLN9y"
      },
      "source": [
        "from re import sub\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "query_string = 'fruit and vegetables'\n",
        "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
        "\n",
        "stopwords = ['the', 'and', 'are', 'a']\n",
        "\n",
        "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
        "def preprocess(doc):\n",
        "    # Tokenize, clean up input document string\n",
        "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
        "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
        "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
        "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
        "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n",
        "\n",
        "# Preprocess the documents, including the query string\n",
        "corpus = [preprocess(document) for document in documents]\n",
        "query = preprocess(query_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx15cgo8LN9y",
        "outputId": "dc840725-bfc8-4013-ecf6-65faae208863"
      },
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.models import WordEmbeddingSimilarityIndex\n",
        "from gensim.similarities import SparseTermSimilarityMatrix\n",
        "from gensim.similarities import SoftCosineSimilarity\n",
        "\n",
        "# Load the model: this is a big file, can take a while to download and open\n",
        "glove = api.load(\"glove-wiki-gigaword-50\")    \n",
        "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
        "\n",
        "# Build the term dictionary, TF-idf model\n",
        "dictionary = Dictionary(corpus+[query])\n",
        "tfidf = TfidfModel(dictionary=dictionary)\n",
        "\n",
        "# Create the term similarity matrix.  \n",
        "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2fRo2OOLN9y",
        "outputId": "d94aa98d-ba2d-4c2a-920f-a9d5bb587913"
      },
      "source": [
        "import numpy as np\n",
        "query_tf = tfidf[dictionary.doc2bow(query)]\n",
        "\n",
        "index = SoftCosineSimilarity(\n",
        "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
        "            similarity_matrix)\n",
        "\n",
        "doc_similarity_scores = index[query_tf]\n",
        "\n",
        "# Output the sorted similarity scores and documents\n",
        "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
        "for idx in sorted_indexes:\n",
        "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {documents[idx]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 \t 0.688 \t tomatoes are actually fruit\n",
            "0 \t 0.000 \t cars drive on the road\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxuG53OELN9z"
      },
      "source": [
        "Conclusion: We learned the use of various text similarity prediction algorithms"
      ]
    }
  ]
}