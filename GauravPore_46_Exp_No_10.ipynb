{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SanketChavan_05_Exp_No_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjNIjWX17yrR"
      },
      "source": [
        "Aim: Write a program to generate feature vector of text using Bag of words and TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpTc4kh273VP"
      },
      "source": [
        "Theory :\n",
        "Bag of words\n",
        "Bag of Words (BOW) is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set.\n",
        "\n",
        "In simple terms, it’s a collection of words to represent a sentence with word count and mostly disregarding the order in which they appear.\n",
        "\n",
        "BOW is an approach widely used with:\n",
        "\n",
        "1.Natural language processing 2.Information retrieval from documents 3.Document classifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52K6pnXR76RS"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohTyQ7p78Cyg",
        "outputId": "e1f4df76-935f-465f-b100-6ffed253ab9d"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA113oaw8EiW"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdERlNXL8IKj"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is_URHkO8OM8"
      },
      "source": [
        "def vectorize(tokens):\n",
        "    vector=[]\n",
        "    for w in filtered_vocab:\n",
        "        vector.append(tokens.count(w))\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGSXI1y78P9i"
      },
      "source": [
        "def unique(sequence):\n",
        "    seen = set()\n",
        "    return [x for x in sequence if not (x in seen or seen.add(x))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzugxdSE8Ugo"
      },
      "source": [
        "stopwords=[\"to\",\"is\",\"a\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcvAfwzP8Y7S"
      },
      "source": [
        "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9j-di2J8dWl"
      },
      "source": [
        "string1=\"Welcome to Great Learning , Now start learning\"\n",
        "string2=\"Learning is a good practice\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vadkxHBX8hrt",
        "outputId": "7847443f-0876-4425-a616-0b9970e5a5d8"
      },
      "source": [
        "string1=string1.lower()\n",
        "string2=string2.lower()\n",
        "\n",
        "tokens1=string1.split()\n",
        "tokens2=string2.split()\n",
        "print(tokens1)\n",
        "print(tokens2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
            "['learning', 'is', 'a', 'good', 'practice']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akrLxqVi8lL6",
        "outputId": "e120f11f-de07-4b95-c5cc-bb66e79d9333"
      },
      "source": [
        "vocab=unique(tokens1+tokens2)\n",
        "print(vocab)\n",
        "filtered_vocab=[]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96WU2L9K8ojS",
        "outputId": "ca16e625-f352-441d-a1a8-8f18570645f8"
      },
      "source": [
        "filtered_vocab=[]\n",
        "for w in vocab: \n",
        "    if w not in stopwords and w not in special_char: \n",
        "        filtered_vocab.append(w)\n",
        "print(filtered_vocab)\n",
        "#convert sentences into vectords\n",
        "vector1=vectorize(tokens1)\n",
        "print(vector1)\n",
        "vector2=vectorize(tokens2)\n",
        "print(vector2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
            "[1, 1, 2, 1, 1, 0, 0]\n",
            "[0, 0, 1, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpdLlhZy8urG"
      },
      "source": [
        "TF-IDF\n",
        "TF-IDF is a method which gives us a numerical weightage of words which reflects how important the particular word is to a document in a corpus. A corpus is a collection of documents. Tf is Term frequency, and IDF is Inverse document frequency. This method is often used for information retrieval and text mining.\n",
        "\n",
        "Tf(Term Frequency): Term frequency can be thought of as how often does a word ‘w’ occur in a document ‘d’. More importance is given to words frequently occurring in a document. The formula of Term frequency is:\n",
        "\n",
        "IDF(inverse document frequency): Sometimes, words like ‘the’ occur a lot and do not give us vital information regarding the document. To minimize the weight of terms occurring very frequently by incorporating the weight of words rarely occurring in the document. In other words, idf is used to calculate rare words’ weight across all documents in corpus. Words rarely occurring in the corpus will have higher IDF values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VGTsV_s8z7o"
      },
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn5lD-nX832c"
      },
      "source": [
        "corpus = [\n",
        "      'this is the first document',\n",
        "      'this document is the second document',\n",
        "      'and this is the third one',\n",
        "      'is this the first document',\n",
        " ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf9i1Air87-l"
      },
      "source": [
        "def IDF(corpus, unique_words):\n",
        "   idf_dict={}\n",
        "   N=len(corpus)\n",
        "   for i in unique_words:\n",
        "     count=0\n",
        "     for sen in corpus:\n",
        "       if i in sen.split():\n",
        "         count=count+1\n",
        "       idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
        "   return idf_dict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY0Gvp3L9CGT"
      },
      "source": [
        "def fit(whole_data):\n",
        "    unique_words = set()\n",
        "    if isinstance(whole_data, (list,)):\n",
        "      for x in whole_data:\n",
        "        for y in x.split():\n",
        "          if len(y)<2:\n",
        "            continue\n",
        "          unique_words.add(y)\n",
        "      unique_words = sorted(list(unique_words))\n",
        "      vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "      Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
        "    return vocab, Idf_values_of_all_unique_words\n",
        "Vocabulary, idf_of_vocabulary=fit(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4FEoPi39Fwn",
        "outputId": "e2c47138-eb4e-4298-9258-0be487c263b7"
      },
      "source": [
        "print(list(Vocabulary.keys())) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABLg01Ct9Iy1",
        "outputId": "7cf9dfa0-7b70-4fa6-83c1-2b65f1772fd6"
      },
      "source": [
        "print(list(idf_of_vocabulary.values()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqkp5Lsn9L7X"
      },
      "source": [
        "Conclusion:Thus we successfully executed the program to generate feature vector of text using Bag of words and TF-IDF"
      ]
    }
  ]
}