{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SanketChavan_05_Exp_No_11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WdcmJcr0vdr"
      },
      "source": [
        "AIM: WRITE A PROGRAM TO GENERATE WORD EMBEDDING OF THE TEXT USING WORD2VEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_2In1nW01U3"
      },
      "source": [
        "THEORY: Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrix, probabilistic models, etc.\n",
        "Word2Vec consists of models for generating word embedding. These models are shallow two layer neural networks having one input layer, one hidden layer and one output layer. Word2Vec utilizes two architectures :\n",
        "1. CBOW (Continuous Bag of Words) : CBOW model predicts the current word given context words within specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer.\n",
        "2. Skip Gram : Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MOTtkbW0861"
      },
      "source": [
        "EXECUTION:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSVMzbLM1Ama",
        "outputId": "74bb9ab2-b389-4c7f-8501-fbc2f7d69f30"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import gensim\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "  \n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "sample = open(\"Sample_text.txt\", \"r\")\n",
        "s = sample.read()\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "  \n",
        "data = []\n",
        "\n",
        "for i in sent_tokenize(f):\n",
        "    temp = []\n",
        "\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        "  \n",
        "    data.append(temp)\n",
        "\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5)\n",
        "\n",
        "print(\"Cosine similarity between 'language' \" + \"and 'processing' - CBOW : \",model1.similarity('language', 'processing'))\n",
        "      \n",
        "print(\"Cosine similarity between 'language' \" + \"and 'processing' - CBOW : \", model1.similarity('language', 'processing'))\n",
        "\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5, sg = 1)\n",
        "\n",
        "print(\"Cosine similarity between 'language' \" +\"and 'processing' - Skip Gram : \",model2.similarity('language', 'processing'))\n",
        "      \n",
        "print(\"Cosine similarity between 'language' \" +\"and 'processing' - Skip Gram : \",model2.similarity('language', 'processing'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Cosine similarity between 'language' and 'processing' - CBOW :  -0.14855781\n",
            "Cosine similarity between 'language' and 'processing' - CBOW :  -0.14855781\n",
            "Cosine similarity between 'language' and 'processing' - Skip Gram :  -0.14400575\n",
            "Cosine similarity between 'language' and 'processing' - Skip Gram :  -0.14400575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf2F4nxV1qQ5"
      },
      "source": [
        "OUTPUT:\n",
        "Cosine similarity between 'language' and 'processing' - CBOW : 0.083356075\n",
        "Cosine similarity between 'language' and 'processing' - CBOW : 0.083356075\n",
        "Cosine similarity between 'language' and 'processing' - Skip Gram : 0.09677577\n",
        "Cosine similarity between 'language' and 'processing' - Skip Gram : 0.09677577"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrxHn07T1ubc"
      },
      "source": [
        "CONCLUSION:\n",
        "Hence we studied word embedding of the text using Word2Vec."
      ]
    }
  ]
}